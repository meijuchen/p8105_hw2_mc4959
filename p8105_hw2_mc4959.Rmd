---
title: "p8105_hw2_mc4959"
author: "Meiju Chen"
date: "9/30/2020"
output: github_document
---

```{r setup, echo=FALSE}
library(tidyverse)
library(readxl)
```

## Problem 1

Read and clean the Mr. Trashwheel dataset.

```{r}
trashwheel_df = 
  read_xlsx(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "Mr. Trash Wheel",
    range = cell_cols("A:N")) %>%
  janitor::clean_names() %>%
  drop_na(dumpster) %>%
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
  )
```

Read and clean precipitation data for 2018 and 2017.

```{r}
precip_2018 = 
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "2018 Precipitation",
    skip = 1,
  ) %>%
  janitor::clean_names() %>%
  drop_na(month) %>%
  mutate(year = 2018) %>%
  relocate(year)

precip_2017 = 
  read_excel(
    "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "2017 Precipitation",
    skip = 1,
  ) %>%
  janitor::clean_names() %>%
  drop_na(month) %>%
  mutate(year = 2017) %>%
  relocate(year)
```

Combine annual precipitation and convert month to a character variable.

```{r}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )

precip_df = 
  bind_rows(precip_2018, precip_2017)

left_join(precip_df, month_df, by = "month")
```

This dataset contains information from the Mr. Trashwheel trash collector in Baltimore, Maryland. As trash enters inner harbor, the trashwheel collects that trash, and stores it in the dumpster. The dataset contains information on year, month, and trash collected, include some specific kinds of trash. There are a total of `r nrow(trashwheel_df)` rows in our final dataset. Additional data sheets include month precipitation data. More info:

* The total precipitation in 2018 was `r precip_df %>% filter(year == 2018) %>% pull(total) %>% sum()` inches.
* The median number of sports balls found in a dumpster in 2017 was `r trashwheel_df %>% filter(year == 2017) %>% pull(sports_balls) %>% median()`




## Problem 2

Read and clean the NYC Transit Dataset.
Retain important variables.
Convert the entry variable to a logical variable.

```{r}
nyc_subway_df = 
  read_csv(
    "./data/nyc_transit_subway_data.csv") %>%
  janitor::clean_names() %>%
  select(line:entry, vending, ada) %>% 
  mutate(entry = recode(entry, YES = "TRUE", NO = "FALSE")) %>% 
  mutate(entry = as.logical(entry))
```

The dataset contains information of entrances and exits for each NYC subway station. I kept some important variables, such as: line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance.

Then I cleaned the dataset, by reformatting the names, and converting the entry variable to a logical vector(True/False).
The dataset has `r nrow(nyc_subway_df)` rows by `r ncol(nyc_subway_df)` columns. However, the dataset is still untidy. It contains lots of NA values and the information of routes are spread across the columns.

* There are `r count(distinct(nyc_subway_df, station_name, line))` distinct stations.
* `r filter(nyc_subway_df, ada == "TRUE") %>% distinct(station_name, line) %>% nrow` stations are ADA compliant.
* The proportion of station entrances/exits without vending allow entrance is `r (filter(nyc_subway_df, vending == "NO" & entry == "TRUE") %>% nrow) / (filter(nyc_subway_df, vending == "NO") %>% nrow)`.


Create a new dataset that tidies up the previous one, and reformat the data so that the route number and route name are distinct variables.

```{r}
tidy_subway_df =
  nyc_subway_df %>%
  mutate_at(vars(route1:route11), as.character) %>%
  pivot_longer(
    route1:route11,
    names_to = "route_name",
    names_prefix = "route",
    values_to = "route_number"
  ) %>% 
  drop_na(route_number)
```

There are `r tidy_subway_df %>% filter(route_number == "A") %>% distinct(line, station_name) %>% count()` stations that serve the A train. Of these stations, only `r tidy_subway_df %>% filter(route_number == "A" & ada == "TRUE") %>% distinct(line, station_name) %>% count()` are ADA compliant.




